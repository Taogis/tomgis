{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "path = '/home/mmtao/test/lanenet-lane-detection/data/tusimple_test_image/05250538_0313.MP4/00000.jpg'\n",
    "label_path = '/home/mmtao/data/CULane/laneseg_label_w16/driver_23_30frame/05151640_0419.MP4/00000.png'\n",
    "image = cv2.cvtColor(cv2.imread(path,-1), cv2.COLOR_BGR2RGB)\n",
    "label_in = cv2.imread(label_path,cv2.IMREAD_COLOR)\n",
    "ret, label = cv2.threshold(label_in,0,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "#cv2.imshow(\"dd\",image)\n",
    "#cv2.waitKey(2000)\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "crop_height = 576\n",
    "crop_width = 1600\n",
    "        \n",
    "if (crop_width <= image.shape[1]) and (crop_height <= image.shape[0]):\n",
    "    x = random.randint(0, image.shape[1]-crop_width)\n",
    "    y = random.randint(0, image.shape[0]-crop_height)\n",
    "    \n",
    "    if len(label.shape) == 3:\n",
    "        image_result = image[y:y+crop_height, x:x+crop_width, :]\n",
    "        label_result = label[y:y+crop_height, x:x+crop_width, :]\n",
    "    else:\n",
    "        image_result = image[y:y+crop_height, x:x+crop_width, :]\n",
    "        label_result = label[y:y+crop_height, x:x+crop_width]\n",
    "else:\n",
    "    Exception('Crop shape (%d, %d) exceeds image dimensions (%d, %d)!' % (crop_height, crop_width, image.shape[0], image.shape[1]))\n",
    "    \n",
    "cv2.imshow(\"result1\",image_result)\n",
    "cv2.imshow(\"result2\",label_result)\n",
    "cv2.waitKey(30000)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/mmtao/test/lanenet-lane-detection/data/tusimple_test_image/05250538_0313.MP4/00000.jpg'\n",
    "image = cv2.cvtColor(cv2.imread(path,-1), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.1\n"
     ]
    }
   ],
   "source": [
    "a = 10\n",
    "b = 91\n",
    "print(b/a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.14419348e-06 3.04858735e+00 2.12692801e+00]\n",
      " [3.04858735e+00 4.53988992e-05 1.31326169e+00]\n",
      " [1.31326169e+00 2.12692801e+00 6.71534849e-03]\n",
      " [1.81499279e-02 1.50231016e-03 1.46328247e+00]\n",
      " [3.04858735e+00 2.47568514e-03 1.31326169e+00]]\n",
      "(5, 3)\n",
      "5\n",
      "(5, 3)\n",
      "-------------\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:3 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-251f1901d181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#sess.run(tf.global_variables_initializer())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:3 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "\n",
    "y = np.array([[1,0,0],[0,1,0],[0,0,1],[1,1,0],[0,1,0]])\n",
    "\n",
    "logits = np.array([[12,3,2],[3,10,1],[1,2,5],[4,6.5,1.2],[3,6,1]])\n",
    "y_pred = sigmoid(logits)\n",
    "E1 = -y*np.log(y_pred)-(1-y)*np.log(1-y_pred)\n",
    "print(E1)\n",
    "print(y.shape)\n",
    "print(np.shape(logits)[0])\n",
    "print(E1.shape)\n",
    "print('-------------')\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    sess =tf.Session()\n",
    "    y = np.array(y).astype(np.float64)\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    E2 = sess.run(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=logits))\n",
    "    print(E2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.  3.  2.]\n",
      "[ 3. 10.  1.]\n",
      "[1. 2. 5.]\n",
      "[4.  6.5 1.2]\n",
      "[3. 6. 1.]\n",
      "[1.68795487e-04 1.03475622e-03 6.58839038e-02 2.58349207e+00\n",
      " 5.49852354e-02]\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:3 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ec43029d7052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mE1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mE2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:3 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    sum_raw = np.sum(np.exp(x),axis=-1)\n",
    "    x1 = np.ones(np.shape(x))\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        print(x[i])\n",
    "        x1[i] = np.exp(x[i])/sum_raw[i]\n",
    "    return x1\n",
    "\n",
    "y = np.array([[1,0,0],[0,1,0],[0,0,1],[1,0,0],[0,1,0]])\n",
    "\n",
    "logits =np.array([[12,3,2],[3,10,1],[1,2,5],[4,6.5,1.2],[3,6,1]])\n",
    "\n",
    "y_pred =softmax(logits)\n",
    "E1 = -np.sum(y*np.log(y_pred),-1)\n",
    "print(E1)\n",
    "sess = tf.Session()\n",
    "y = np.array(y).astype(np.float64)\n",
    "E2 = sess.run(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits))\n",
    "print(E2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.68795487e-04 1.03475622e-03 6.58839038e-02 2.58349207e+00\n",
      " 5.49852354e-02]\n",
      "[[9.99831219e-01 1.23388975e-04 4.53922671e-05]\n",
      " [9.10938878e-04 9.98965779e-01 1.23282171e-04]\n",
      " [1.71478255e-02 4.66126226e-02 9.36239552e-01]\n",
      " [7.55098575e-02 9.19898383e-01 4.59175917e-03]\n",
      " [4.71234165e-02 9.46499123e-01 6.37746092e-03]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function for each row of the input x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A N dimensional vector or M x N dimensional numpy matrix.\n",
    "\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        exp_minmax = lambda x: np.exp(x - np.max(x))\n",
    "        denom = lambda x: 1.0 / np.sum(x)\n",
    "        x = np.apply_along_axis(exp_minmax, 1, x)\n",
    "        denominator = np.apply_along_axis(denom, 1, x)\n",
    "\n",
    "        if len(denominator.shape) == 1:\n",
    "            denominator = denominator.reshape((denominator.shape[0], 1))\n",
    "\n",
    "        x = x * denominator\n",
    "    else:\n",
    "        # Vector\n",
    "        x_max = np.max(x)\n",
    "        x = x - x_max\n",
    "        numerator = np.exp(x)\n",
    "        denominator = 1.0 / np.sum(numerator)\n",
    "        x = numerator.dot(denominator)\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x\n",
    "\n",
    "y = np.array([[1,0,0],[0,1,0],[0,0,1],[1,0,0],[0,1,0]])\n",
    "\n",
    "logits =np.array([[12,3,2],[3,10,1],[1,2,5],[4,6.5,1.2],[3,6,1]])\n",
    "\n",
    "y_pred =softmax(logits)\n",
    "E1 = -np.sum(y*np.log(y_pred),-1)\n",
    "print(E1)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "path = '/home/mmtao/test/Semantic-Segmentation-Suite/CamVid/train/0001TP_006690.png'\n",
    "\n",
    "#image = cv2.cvtColor(cv2.imread(path,-1), cv2.COLOR_BGR2RGB)\n",
    "image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "\n",
    "cv2.imshow(\"dd\",image)\n",
    "cv2.waitKey(9000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................\n",
      "1.0\n",
      "0.025\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.024924987484695432\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.024849949877348225\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.024774887085320857\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.02469979901532068\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.02462468557339305\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.024549546664914382\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.024474382194585094\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.024399192066422437\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.024323976183753263\n",
      ".................\n",
      "1.0\n",
      "0.0242488097038868\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.024176051135525584\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.024103268229077688\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.02403046089468912\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.023957629041870467\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.023884772579490184\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.023811891415767908\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.02373898545826757\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.02366605461389049\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.0235930987888683\n",
      ".................\n",
      "1.0\n",
      "0.023520190882212584\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.02344961853507185\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.023379022581145815\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.023308402933280393\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.02323775950370515\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.023167092204026834\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.023096400945222865\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.023025685637634654\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.022954946190960904\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.02288418251425078\n",
      ".................\n",
      "1.0\n",
      "0.02281346531607465\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.02274501349942776\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.022676538785323087\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.022608041089225324\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.022539520326001324\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.022470976409913843\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.022402409254615216\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.02233381877314088\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.022265204877902883\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.022196567480683237\n",
      ".................\n",
      "1.0\n",
      "0.022127975165428634\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.0220615801623984\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.02199516294992431\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.02192872344601114\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.021862261568083782\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.021795777232981203\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.02172927035695027\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.021662740855639507\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.02159618864409276\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.02152961363674279\n",
      ".................\n",
      "1.0\n",
      "0.021463082356752476\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.02139868236500171\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.021334260831147842\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.021269817675659393\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.021205352818442456\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.02114086617883478\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.02107635767559982\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.02101182722692065\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.020947274750393857\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.020882700163023283\n",
      ".................\n",
      "1.0\n",
      "0.02081816798910964\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.020755703063313796\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.020693217242703593\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.0206307104501373\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.02056818260792763\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.020505633637836043\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.020443063461066956\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.020380471998261852\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.02031785916949334\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.020255224894259096\n",
      ".................\n",
      "1.0\n",
      "0.020192631758058695\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.02013204375410706\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.02007143548318034\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.020010806870454725\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.01995015784057725\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.01988948831766025\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.01982879822527576\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.019768087486449808\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.019707356023656617\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.01964660375881278\n",
      ".................\n",
      "1.0\n",
      "0.019585891396873056\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.019527123917746594\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.01946833678061925\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.019409529912915485\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.019350703241546516\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.019291856692904917\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.01923299019285921\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.01917410366674829\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.01911519703937586\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.019056270235004724\n",
      ".................\n",
      "1.0\n",
      "0.018997382134550732\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.018940380477826143\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.01888335975377665\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.01882631989200744\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.01876926082162587\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.01871218247123625\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.01865508476893456\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.018597967642303096\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.01854083101840502\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.01848367482377887\n",
      ".................\n",
      "1.0\n",
      "0.01842655617010958\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.018371267277040748\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.018315959889574587\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.018260633939431486\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.01820528935784895\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.01814992607557656\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.018094544022870845\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.01803914312949008\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.017983723324689\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.017928284537213485\n",
      ".................\n",
      "1.0\n",
      "0.01787288216267873\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.017819254568808143\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.017765609036252704\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.01771194549878443\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.017658263889706986\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.017604564141850753\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.017550846187567897\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.017497109958727275\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.01744335538670936\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.01738958240240104\n",
      ".................\n",
      "1.0\n",
      "0.01733584473691157\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.01728382852416576\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.01723179491174974\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.017179743835425518\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.017127675230500822\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.017075589031824323\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.01702348517378084\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.01697136359028641\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.016919224214783358\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.016867066980235254\n",
      ".................\n",
      "1.0\n",
      "0.016814944003259846\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.016764490753484247\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.01671402062685697\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.016663533561070226\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.016613029493375595\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.016562508360579386\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.016511970099037994\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.016461414644653127\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.016410841932867005\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.016360251898657488\n",
      ".................\n",
      "1.0\n",
      "0.01630969509266242\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.016260757842552372\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.016211804222699737\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.016162834172668922\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.01611384763159695\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.016064844538188943\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.016015824830713645\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.015966788446998757\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.015917735324426315\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.015868665399927925\n",
      ".................\n",
      "1.0\n",
      "0.015819627705215493\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.01577216090260149\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.01572467822211657\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.015677179605141093\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.015629664992640853\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.015582134325162755\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.015534587542830406\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.015487024585339643\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.015439445391954017\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.015391849901500197\n",
      ".................\n",
      "1.0\n",
      "0.015344285672404236\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.01529824513385068\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.015252189194518309\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.01520611779754886\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.015160030885681974\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.015113928401250967\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.015067810286178597\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.015021676481972696\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.014975526929721795\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.014929361570090682\n",
      ".................\n",
      "1.0\n",
      "0.01488322653248828\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.014838569402166294\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.01479389733382212\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.014749210272305942\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.014704508162077945\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.01465979094720419\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.014615058571352516\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.014570310977788303\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.01452554810937024\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.014480769908545982\n",
      ".................\n",
      "1.0\n",
      "0.01443602111864589\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.014392705828441911\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.014349376049067632\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.014306031727030367\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.014262672808459132\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.014219299239100671\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.014175910964315471\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.014132507929073654\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.014089090077950863\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.01404565735512407\n",
      ".................\n",
      "1.0\n",
      "0.014002253159493409\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.013960239390316412\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.01391821156733408\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.013876169638661055\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.013834113552045047\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.013792043254862982\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.013749958694117131\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.013707859816431134\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.013665746568046004\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.013623618894816062\n",
      ".................\n",
      "1.0\n",
      "0.013581518891608134\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.013540767535859493\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.013500002548588805\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.013459223879469741\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.013418431477820071\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.013377625292597917\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.013336805272398005\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.013295971365447806\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.013255123519603665\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.01321426168234685\n",
      ".................\n",
      "1.0\n",
      "0.013173426683693966\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.013133899808864999\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.013094359712108667\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.013054806344610832\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.013015239657212135\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.012975659600404392\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.012936066124326934\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.01289645917876287\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.012856838713135338\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.01281720467650367\n",
      ".................\n",
      "1.0\n",
      "0.012777596672039981\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.012739257485403301\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.012700905474126601\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.012662540590862482\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.012624162787928717\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.012585772017304717\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.01254736823062801\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.012508951379190602\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.012470521413935346\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.012432078285452218\n",
      ".................\n",
      "1.0\n",
      "0.01239366040693259\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.012356473221294404\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.012319273596366002\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.012282061486222667\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.012244836844614899\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.012207599624965022\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.012170349780363736\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.012133087263566609\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.012095812026990541\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.012058524022710165\n",
      ".................\n",
      "1.0\n",
      "0.012021260509692185\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.011985190710173645\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.011949108845135854\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.01191301487003402\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.011876908740008336\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.011840790409880668\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.01180465983415123\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.011768516966995173\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.011732361762259155\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.011696194173457852\n",
      ".................\n",
      "1.0\n",
      "0.011660050340015\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.011625064351831704\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.011590066660669194\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.011555057223321147\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.011520035996275681\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.011485002935712162\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.011449957997497963\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.011414901137185159\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.01137983231000721\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.011344751470875561\n",
      ".................\n",
      "1.0\n",
      "0.011309693673310571\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.011275758930520205\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.0112418128363972\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.011207855349033482\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.011173886426224598\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.011139906025466616\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.011105914103952993\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.011071910618571356\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.011037895525900293\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.011003868782206052\n",
      ".................\n",
      "1.0\n",
      "0.01096986438773442\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.010936949302923462\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.010904023207860304\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.010871086061896107\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.010838137824094561\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.010805178453228877\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.010772207907778743\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.010739226145927216\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.010706233125557588\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.010673228804250205\n",
      ".................\n",
      "1.0\n",
      "0.010640246160624665\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.010608320095505936\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.010576383350966792\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.010544435887579792\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.01051247766563867\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.010480508645155397\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.010448528785857253\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.010416538047183788\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.010384536388283806\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.010352523768012259\n",
      ".................\n",
      "1.0\n",
      "0.010320532174059985\n",
      ".................\n",
      "0.9969994993878173\n",
      "0.010289565410953667\n",
      ".................\n",
      "0.9939979950939289\n",
      "0.010258588289318013\n",
      ".................\n",
      "0.9909954834128342\n",
      "0.010227600770910283\n",
      ".................\n",
      "0.9879919606128271\n",
      "0.010196602817217288\n",
      ".................\n",
      "0.9849874229357218\n",
      "0.010165594389452548\n",
      ".................\n",
      "0.9819818665965753\n",
      "0.010134575448553436\n",
      ".................\n",
      "0.9789752877834037\n",
      "0.010103545955178251\n",
      ".................\n",
      "0.9759676826568975\n",
      "0.010072505869703275\n",
      ".................\n",
      "0.9729590473501305\n",
      "0.010041455152219774\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "epoch_start_i = 0\n",
    "num_epochs = 30\n",
    "num_iters = 10000\n",
    "learning_rate_init = 0.025\n",
    "\n",
    "for epoch in range(epoch_start_i, num_epochs):\n",
    "    iter_index = 0\n",
    "    for i in range(num_iters):\n",
    "        # st=time.time()\n",
    "        b = (1.0-(i+num_iters*iter_index)/((num_epochs-epoch_start_i)*num_iters))**(0.9)\n",
    "        learning_rate_update = learning_rate_init*b\n",
    "        #print(learning_rate_update)\n",
    "        #print((1.0-(i+num_iters*iter_index)/((num_epochs-epoch_start_i)*num_iters))**(0.9))\n",
    "        if i%1000 ==0:\n",
    "            print('.................')\n",
    "            print(b)\n",
    "            print(learning_rate_update)\n",
    "        #learning_rate_init = learning_rate_update\n",
    "    iter_index+=1\n",
    "    learning_rate_init = learning_rate_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  2  3  4  5]\n",
      "  [ 6  7  8  9 10]]\n",
      "\n",
      " [[11 12 13 14 15]\n",
      "  [16 17 18 19 20]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "temp4=tf.reshape(tf.range(0,20)+tf.constant(1,shape=[20]),[2,2,5])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(temp4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
